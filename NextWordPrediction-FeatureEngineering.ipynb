{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\aayus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\aayus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#conda install gensim or pip install gensim\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import functools\n",
    "import gensim\n",
    "from functools import reduce\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./emailData/emails.csv\"\n",
    "# Read the data into a pandas dataframe called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "headers = [header for header in emails.columns]\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "print(display(emails.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
      "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tim.belden@enron.com\n",
      "Subject: \n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "Here is our forecast\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(emails.loc[0][\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved header information!\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "\n",
    "def insert_value(dictionary, key, value):\n",
    "    if key in dictionary:\n",
    "        values = dictionary.get(key)\n",
    "        values.append(value)\n",
    "        dictionary[key] = values\n",
    "    else:\n",
    "        dictionary[key] = [value]\n",
    "    return dictionary\n",
    "\n",
    "def get_headers(df, header_names):\n",
    "    headers = {}\n",
    "    messages = df[\"message\"]\n",
    "    for message in messages:\n",
    "        e = email.message_from_string(message)\n",
    "        for item in header_names:\n",
    "            header = e.get(item)\n",
    "            insert_value(dictionary = headers, key = item, value = header) \n",
    "    print(\"Successfully retrieved header information!\")\n",
    "    return headers\n",
    "header_names = [\"Date\", \"Subject\", \"X-Folder\", \"X-From\", \"X-To\"]    \n",
    "headers = get_headers(emails, header_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    }
   ],
   "source": [
    "def get_messages(df):\n",
    "    messages = []\n",
    "    for item in df[\"message\"]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        message_body = message_body.lower()\n",
    "        messages.append(message_body)\n",
    "    print(\"Successfully retrieved message body from e-mails!\")\n",
    "    return messages\n",
    "msg_body = get_messages(emails)\n",
    "emails[\"Message-Body\"] = msg_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            here is our forecast\\n\\n \n",
       "1    traveling to have a business meeting takes the...\n",
       "2                       test successful.  way to go!!!\n",
       "3    randy,\\n\\n can you send me a schedule of the s...\n",
       "4                  let's shoot for tuesday at 11:45.  \n",
       "5    greg,\\n\\n how about either next tuesday or thu...\n",
       "6    please cc the following distribution list with...\n",
       "7                     any morning between 10 and 11:30\n",
       "8    1. login:  pallen pw: ke9davis\\n\\n i don't thi...\n",
       "9    ---------------------- forwarded by phillip k ...\n",
       "Name: Message-Body, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[\"Message-Body\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens = []\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        #Remove URL\n",
    "        sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "        \n",
    "        #words = nltk.word_tokenize(sentence)\n",
    "        words = tknzr.tokenize(sentence)\n",
    "        #Converting to LowerCase\n",
    "        words = map(str.lower, words)\n",
    "    \n",
    "        words = map(lambda x: re.sub(r'[^a-zA-Z]', \"\", x), words)\n",
    "        \n",
    "        #punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~.?!'\n",
    "        #words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "        \n",
    "        #Remove words which are not part of word2vec vocab\n",
    "        words = filter(lambda x: x in word2vec.vocab, words)\n",
    "        \n",
    "        words = list(words)\n",
    "        \n",
    "        #Add the words to the vocabulary\n",
    "        vocab.update(words)\n",
    "        \n",
    "        # Adding the preprocessed words to the document\n",
    "        tokens = tokens + words\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['message_tokens'] = emails['Message-Body'].apply(lambda x: processText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [here, is, our, forecast]\n",
       "1    [traveling, have, business, meeting, takes, th...\n",
       "2                          [test, successful, way, go]\n",
       "3    [randy, can, you, send, me, schedule, the, sal...\n",
       "4                      [lets, shoot, for, tuesday, at]\n",
       "Name: message_tokens, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['message_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the emailTokens file\n",
    "emails['message_tokens'].to_pickle('./emailTokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the vocab file\n",
    "with open('vocab.txt','w') as f:\n",
    "    f.write(str(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
